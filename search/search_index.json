{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Digital Developer Conference 2021 - Jumpstart your Journey to AI and Data Science \u00b6 Welcome to the course! In this course we'll be using Jupyter notebooks to learn about Python3, use Python and the Pandas library to do some data visualisation, and learn about Python and data science. We use IBM Cloud Pak for Data to run the notebooks and build a Machine Learning model with the scikit learn framework, and we'll demonstrate AutoAI to build ML pipelines, deploy a model and save a notebook based on the exeperiment, and show some basic tests. Agenda \u00b6 Topic Description Time Welcome Introduction to course 00:05 Project setup Setup your project for the workshop 00:10 Getting started with Python3 Learn the basics of Python 00:15 Data Visualization with Matplotlib and Seaborn Use Matplotlib and Seaborn for data visualisation 00:20 Using Pandas DataFrames Use a Pandas DataFrame 00:10 Build a Machine Learning Model with Scikit Learn Build a machine learning model with Scikit Learn 00:20 Build a Machine Learning Model with AutoAI Build a machine learning model with AutoAI 00:15 Deploy and Test Machine Learning Model with AutoAI Deploy and test machine learning model with AutoAI 00:15 Claiming your Badge Claim your badge here 00:10 Wrap Up Review of workshop and Preview of next steps 00:05 Compatability \u00b6 This workshop has been tested on IBM Watson Studio and Cloud Pak for Data as a Service The notebooks use Python 3.7.10 .","title":"About the workshop"},{"location":"#digital-developer-conference-2021-jumpstart-your-journey-to-ai-and-data-science","text":"Welcome to the course! In this course we'll be using Jupyter notebooks to learn about Python3, use Python and the Pandas library to do some data visualisation, and learn about Python and data science. We use IBM Cloud Pak for Data to run the notebooks and build a Machine Learning model with the scikit learn framework, and we'll demonstrate AutoAI to build ML pipelines, deploy a model and save a notebook based on the exeperiment, and show some basic tests.","title":"Digital Developer Conference 2021 - Jumpstart your Journey to AI and Data Science"},{"location":"#agenda","text":"Topic Description Time Welcome Introduction to course 00:05 Project setup Setup your project for the workshop 00:10 Getting started with Python3 Learn the basics of Python 00:15 Data Visualization with Matplotlib and Seaborn Use Matplotlib and Seaborn for data visualisation 00:20 Using Pandas DataFrames Use a Pandas DataFrame 00:10 Build a Machine Learning Model with Scikit Learn Build a machine learning model with Scikit Learn 00:20 Build a Machine Learning Model with AutoAI Build a machine learning model with AutoAI 00:15 Deploy and Test Machine Learning Model with AutoAI Deploy and test machine learning model with AutoAI 00:15 Claiming your Badge Claim your badge here 00:10 Wrap Up Review of workshop and Preview of next steps 00:05","title":"Agenda"},{"location":"#compatability","text":"This workshop has been tested on IBM Watson Studio and Cloud Pak for Data as a Service The notebooks use Python 3.7.10 .","title":"Compatability"},{"location":"SUMMARY/","text":"Summary \u00b6 Project setup \u00b6 Perform the steps to setup your project Python 3 \u00b6 Getting started with Python3 Pandas \u00b6 Using Python and Pandas for data visualization Course Badge \u00b6 Claim your badge Resources \u00b6 Watson Studio Python3 tutorial Introduction to Python Markdown Syntax","title":"Summary"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#project-setup","text":"Perform the steps to setup your project","title":"Project setup"},{"location":"SUMMARY/#python-3","text":"Getting started with Python3","title":"Python 3"},{"location":"SUMMARY/#pandas","text":"Using Python and Pandas for data visualization","title":"Pandas"},{"location":"SUMMARY/#course-badge","text":"Claim your badge","title":"Course Badge"},{"location":"SUMMARY/#resources","text":"Watson Studio Python3 tutorial Introduction to Python Markdown Syntax","title":"Resources"},{"location":"autoai-build/","text":"Machine Learning Models with Auto AI \u00b6 In this workshop you will learn how to build and deploy your own AI Models. For the workshop we will be using AutoAI, a graphical tool that analyses your dataset and discovers data transformations, algorithms, and parameter settings that work best for your problem setting. Using AutoAI, you can build and deploy a machine learning model with sophisticated training features and no coding. We will a public dataset to build and deploy model pipelines, and analyse the outcome. Set up \u00b6 Make sure that you have followed the steps in the project setup to create a project and set up your IBM Cloud account and Cloud Pak for Data as a Service. Once you are in the Project Dashboard, click on \"Add to Project\" on the top right and select AutoAI Experiment \u00b6 Associate a Machine Learning service \u00b6 Give your Auto AI experiment a unique name Associate a Watson Machine Learning service, if you have already created one this will apear in the dropdown or you can create a new one. Once this is done, click the \"Reload\" button for your Machine Learning service to appear Your machine learning service will appear under \"Associated services\" Click Create Upload your Data Sets \u00b6 Browse and add your Data source. You will have already added insurance.csv to your project assets in the project setup steps. Click Select from project and choose the insurance.csv file. Once your dataset is successfully uploaded, you will see an option to choose your prediction column . Optionally, you can also refer to the experiment settings to make changes to the AutoAI Experiment. Once done, click on Run Experiment. Completed AutoAI experiment \u00b6 The experiment will take approximagely 20 minutes to run. You can check on the progress during that time. When completed, it will show the results of the exeriment. In the next module, we will work through the different aspects of configuring our AutoAI experiment and discuss the outcome of the experiment. \u00b6 Deploying the model \u00b6","title":"Build and Deploy Custom AI Predictive Models Part 1 - Intro & Building"},{"location":"autoai-build/#machine-learning-models-with-auto-ai","text":"In this workshop you will learn how to build and deploy your own AI Models. For the workshop we will be using AutoAI, a graphical tool that analyses your dataset and discovers data transformations, algorithms, and parameter settings that work best for your problem setting. Using AutoAI, you can build and deploy a machine learning model with sophisticated training features and no coding. We will a public dataset to build and deploy model pipelines, and analyse the outcome.","title":"Machine Learning Models with Auto AI"},{"location":"autoai-build/#set-up","text":"Make sure that you have followed the steps in the project setup to create a project and set up your IBM Cloud account and Cloud Pak for Data as a Service.","title":"Set up"},{"location":"autoai-build/#once-you-are-in-the-project-dashboard-click-on-add-to-project-on-the-top-right-and-select-autoai-experiment","text":"","title":"Once you are in the Project Dashboard, click on \"Add to Project\" on the top right and select AutoAI Experiment"},{"location":"autoai-build/#associate-a-machine-learning-service","text":"Give your Auto AI experiment a unique name Associate a Watson Machine Learning service, if you have already created one this will apear in the dropdown or you can create a new one. Once this is done, click the \"Reload\" button for your Machine Learning service to appear Your machine learning service will appear under \"Associated services\" Click Create","title":"Associate a Machine Learning service"},{"location":"autoai-build/#upload-your-data-sets","text":"Browse and add your Data source. You will have already added insurance.csv to your project assets in the project setup steps. Click Select from project and choose the insurance.csv file. Once your dataset is successfully uploaded, you will see an option to choose your prediction column . Optionally, you can also refer to the experiment settings to make changes to the AutoAI Experiment. Once done, click on Run Experiment.","title":"Upload your Data Sets"},{"location":"autoai-build/#completed-autoai-experiment","text":"The experiment will take approximagely 20 minutes to run. You can check on the progress during that time. When completed, it will show the results of the exeriment.","title":"Completed AutoAI experiment"},{"location":"autoai-build/#in-the-next-module-we-will-work-through-the-different-aspects-of-configuring-our-autoai-experiment-and-discuss-the-outcome-of-the-experiment","text":"","title":"In the next module, we will work through the different aspects of configuring our AutoAI experiment and discuss the outcome of the experiment."},{"location":"autoai-build/#deploying-the-model","text":"","title":"Deploying the model"},{"location":"autoai-deploy/","text":"Deploying your Model \u00b6 This is the second AutoAI module, and assumes that you have already created and ran the AutoAI experiment . Once you select your final model pipeline. You can choose to Save it as a notebook or an ML model. Choose model and click save Once your model is saved you will see a popup to View in Project or you can access the saved model from the Assets tab. Click on the saved model and Promote to a deployment space Select the target deployment space you want to use for this deployment. If you do not have one yet, you can create a new space. Access the deployment space from the left navigation pane. In your Deployment Space, Choose the model you want to deploy from the Assets tab and click the deploy icon as shown Choose your Deployment type and create your Deployment. Once your model is successuly deployed, you can access the API reference to make API calls or Test your model by providing input using a form or in a JSON format Save the model as a Jupyter notebook \u00b6 Next we will Save your pipeline as a notebook","title":"Build and Deploy Custom AI Predictive Models Part 2 - Deploying & Analyzing"},{"location":"autoai-deploy/#deploying-your-model","text":"This is the second AutoAI module, and assumes that you have already created and ran the AutoAI experiment . Once you select your final model pipeline. You can choose to Save it as a notebook or an ML model. Choose model and click save Once your model is saved you will see a popup to View in Project or you can access the saved model from the Assets tab. Click on the saved model and Promote to a deployment space Select the target deployment space you want to use for this deployment. If you do not have one yet, you can create a new space. Access the deployment space from the left navigation pane. In your Deployment Space, Choose the model you want to deploy from the Assets tab and click the deploy icon as shown Choose your Deployment type and create your Deployment. Once your model is successuly deployed, you can access the API reference to make API calls or Test your model by providing input using a form or in a JSON format","title":"Deploying your Model"},{"location":"autoai-deploy/#save-the-model-as-a-jupyter-notebook","text":"Next we will Save your pipeline as a notebook","title":"Save the model as a Jupyter notebook"},{"location":"autoai-notebook/notebook/","text":"Saving your pipeline as a notebook \u00b6 You can create a notebook if you want to view the code that created the model pipeline or interact with the model programatically. Select the pipeline of your choice and click save as and choose the notebook option. You can choose the notebook type as a WML notebook to work with a trained model in a notebook, review and update the code, view visualisations and also deploy the model with Watson Machine Learning. OR you can choose to save it as a AutoAI_lib notebook where you can view the sci-kit learn source code for the trainel model in the notebook. - Once you click create the notebook will be created for you to access within the project. \u00b6","title":"Build and Deploy Custom AI Predictive Models Part 3 - Creating a notebook"},{"location":"autoai-notebook/notebook/#saving-your-pipeline-as-a-notebook","text":"You can create a notebook if you want to view the code that created the model pipeline or interact with the model programatically. Select the pipeline of your choice and click save as and choose the notebook option. You can choose the notebook type as a WML notebook to work with a trained model in a notebook, review and update the code, view visualisations and also deploy the model with Watson Machine Learning. OR you can choose to save it as a AutoAI_lib notebook where you can view the sci-kit learn source code for the trainel model in the notebook.","title":"Saving your pipeline as a notebook"},{"location":"autoai-notebook/notebook/#-once-you-click-create-the-notebook-will-be-created-for-you-to-access-within-the-project","text":"","title":"- Once you click create the notebook will be created for you to access within the project."},{"location":"claim-your-badge/","text":"Claiming your badge \u00b6 How to claim your badge \u00b6 Congratulations on completing this course, the last step to getting you badge is to complete a short quiz and claim your badge. Here are the step to claiming your badge. Go to the following Cognitive Class course: Data & AI: Jumpstart Your Journey Click \"Login to Enroll\" If you are a first time user, click \"Create an Account\". Otherwise, login with your credentials. Once signed in, expand the Quiz section in the middle of the page and click on the \"badge quiz\" Once you complete the quiz and pass, navigate back the the course page and select the \"Claim Data & AI: Jumpstart Your Journey badge\" section. If you have successfully passed the quiz you will see a \"Claim Badge\" at the bottom of the page. Click the button to claim you badge. That's it! Congratulation on your badge and great job on completing the course! For more learning opportunities, you can take a look at Track 7 of the conference, or other learning content on https://developer.ibm.com","title":"How to claim your badge"},{"location":"claim-your-badge/#claiming-your-badge","text":"","title":"Claiming your badge"},{"location":"claim-your-badge/#how-to-claim-your-badge","text":"Congratulations on completing this course, the last step to getting you badge is to complete a short quiz and claim your badge. Here are the step to claiming your badge. Go to the following Cognitive Class course: Data & AI: Jumpstart Your Journey Click \"Login to Enroll\" If you are a first time user, click \"Create an Account\". Otherwise, login with your credentials. Once signed in, expand the Quiz section in the middle of the page and click on the \"badge quiz\" Once you complete the quiz and pass, navigate back the the course page and select the \"Claim Data & AI: Jumpstart Your Journey badge\" section. If you have successfully passed the quiz you will see a \"Claim Badge\" at the bottom of the page. Click the button to claim you badge. That's it! Congratulation on your badge and great job on completing the course! For more learning opportunities, you can take a look at Track 7 of the conference, or other learning content on https://developer.ibm.com","title":"How to claim your badge"},{"location":"data-exploration-pandas/","text":"Data exploration in Python using pandas \u00b6 What is data preprocessing? \u00b6 Process of converting raw data into useful format.In order to better understand the data, we need to gather some statistical insights into our data. In this module of the course, we will use some of the libraries available with Python and Jupyter to examine our data set. What is pandas? \u00b6 pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Data \u00b6 We'll use a data set from Kaggle for this workshop. You'll need to download it to your local machine, then upload to your project running in Cloud Pak for Data as a Service. The insurance.csv dataset acquired from Kaggle contains 1338 observations (rows) and 7 features (columns). The dataset contains 4 numerical features (age, bmi, children and expenses) and 3 categorical features (sex, smoker and region). We'll continue to use the insurance.csv file from you project assets, so if you have not already downloaded this file to your local machine, and uploaded it to your project, do that now. Getting Started with Jupyter Notebooks \u00b6 Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. Open the Jupyter notebook \u00b6 Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the insurance_premium_pandas_preprocessing.ipynb notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Run the Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Load and Prepare Dataset \u00b6 Section 2.0 Load the data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ). Continue to run the notebook \u00b6 Finish running all of the cells. Carefully read all of the markdown comments to gain some understanding of how data analysis and manipulation can be done to gain insight into the data set.","title":"Using Pandas for data science"},{"location":"data-exploration-pandas/#data-exploration-in-python-using-pandas","text":"","title":"Data exploration in Python using pandas"},{"location":"data-exploration-pandas/#what-is-data-preprocessing","text":"Process of converting raw data into useful format.In order to better understand the data, we need to gather some statistical insights into our data. In this module of the course, we will use some of the libraries available with Python and Jupyter to examine our data set.","title":"What is data preprocessing?"},{"location":"data-exploration-pandas/#what-is-pandas","text":"pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.","title":"What is pandas?"},{"location":"data-exploration-pandas/#data","text":"We'll use a data set from Kaggle for this workshop. You'll need to download it to your local machine, then upload to your project running in Cloud Pak for Data as a Service. The insurance.csv dataset acquired from Kaggle contains 1338 observations (rows) and 7 features (columns). The dataset contains 4 numerical features (age, bmi, children and expenses) and 3 categorical features (sex, smoker and region). We'll continue to use the insurance.csv file from you project assets, so if you have not already downloaded this file to your local machine, and uploaded it to your project, do that now.","title":"Data"},{"location":"data-exploration-pandas/#getting-started-with-jupyter-notebooks","text":"Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text.","title":"Getting Started with Jupyter Notebooks"},{"location":"data-exploration-pandas/#open-the-jupyter-notebook","text":"Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the insurance_premium_pandas_preprocessing.ipynb notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Open the Jupyter notebook"},{"location":"data-exploration-pandas/#run-the-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.","title":"Run the Jupyter notebook"},{"location":"data-exploration-pandas/#load-and-prepare-dataset","text":"Section 2.0 Load the data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ).","title":"Load and Prepare Dataset"},{"location":"data-exploration-pandas/#continue-to-run-the-notebook","text":"Finish running all of the cells. Carefully read all of the markdown comments to gain some understanding of how data analysis and manipulation can be done to gain insight into the data set.","title":"Continue to run the notebook"},{"location":"data-visualization/","text":"Data Visualization in Python using Matplotlib and Seaborn \u00b6 In this module of the course, we will use some of the libraries available with Python and Jupyter to examine our data set. In order to better understand the data, we can use visualizations such as charts, plots, and graphs. We'll use some common tools such as matplotlib and seaborn and gather some statistical insights into our data. We'll continue to use the insurance.csv file from your project assets, so if you have not already downloaded this file to your local machine, and uploaded it to your project, do that now. If you have not already done so, make sure that you do the work for your project setup Getting Started with Jupyter Notebooks \u00b6 Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. Open the Jupyter notebook \u00b6 Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the insurance-expenses-vizualization.ipynb notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Run the Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Load and Prepare Dataset \u00b6 Section 2.0 Load the data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ). Continue to run the notebook \u00b6 Finishing running all of the cells. Carefully read all of the markdown comments to gain some understanding of how data vizualization can be use to gain insight into the data set.","title":"Using Python and Jupyter to Visualize data"},{"location":"data-visualization/#data-visualization-in-python-using-matplotlib-and-seaborn","text":"In this module of the course, we will use some of the libraries available with Python and Jupyter to examine our data set. In order to better understand the data, we can use visualizations such as charts, plots, and graphs. We'll use some common tools such as matplotlib and seaborn and gather some statistical insights into our data. We'll continue to use the insurance.csv file from your project assets, so if you have not already downloaded this file to your local machine, and uploaded it to your project, do that now. If you have not already done so, make sure that you do the work for your project setup","title":"Data Visualization in Python using Matplotlib and Seaborn"},{"location":"data-visualization/#getting-started-with-jupyter-notebooks","text":"Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text.","title":"Getting Started with Jupyter Notebooks"},{"location":"data-visualization/#open-the-jupyter-notebook","text":"Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the insurance-expenses-vizualization.ipynb notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Open the Jupyter notebook"},{"location":"data-visualization/#run-the-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.","title":"Run the Jupyter notebook"},{"location":"data-visualization/#load-and-prepare-dataset","text":"Section 2.0 Load the data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ).","title":"Load and Prepare Dataset"},{"location":"data-visualization/#continue-to-run-the-notebook","text":"Finishing running all of the cells. Carefully read all of the markdown comments to gain some understanding of how data vizualization can be use to gain insight into the data set.","title":"Continue to run the notebook"},{"location":"machine-learning-sklearn/","text":"Machine learning using scikit-learn \u00b6 What is regression in machine learning? \u00b6 Regression is when the feature to be predicted contains continuous values. Regression refers to the process of predicting a dependent variable by analyzing the relationship between other independent variables. There are several algorithms known to us that help us in excavating these relationships to better predict the value. scikit-learn \u00b6 In this notebook, we'll use scikit-learn to predict values. Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. To help visualize what we are doing, we'll use visualizations with matplotlib python library. Data \u00b6 We'll continue to use the insurance.csv file from you project assets, so if you have not already downloaded this file to your local machine, and uploaded it to your project, do that now. Getting Started with Jupyter Notebooks \u00b6 Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. Open the Jupyter notebook \u00b6 Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the predict-insurance-expenses-regression-with-sklearn.ipynb notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Run the Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Load and Prepare Dataset \u00b6 Section 2.0 Load the data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ). Continue to run the notebook \u00b6 Finish running all of the cells. Carefully read all of the markdown comments to gain some understanding of how to build, test and evaluate a machine learning model using sklearn.","title":"Machine Learning with Scikit Learn"},{"location":"machine-learning-sklearn/#machine-learning-using-scikit-learn","text":"","title":"Machine learning using scikit-learn"},{"location":"machine-learning-sklearn/#what-is-regression-in-machine-learning","text":"Regression is when the feature to be predicted contains continuous values. Regression refers to the process of predicting a dependent variable by analyzing the relationship between other independent variables. There are several algorithms known to us that help us in excavating these relationships to better predict the value.","title":"What is regression in machine learning?"},{"location":"machine-learning-sklearn/#scikit-learn","text":"In this notebook, we'll use scikit-learn to predict values. Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. To help visualize what we are doing, we'll use visualizations with matplotlib python library.","title":"scikit-learn"},{"location":"machine-learning-sklearn/#data","text":"We'll continue to use the insurance.csv file from you project assets, so if you have not already downloaded this file to your local machine, and uploaded it to your project, do that now.","title":"Data"},{"location":"machine-learning-sklearn/#getting-started-with-jupyter-notebooks","text":"Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text.","title":"Getting Started with Jupyter Notebooks"},{"location":"machine-learning-sklearn/#open-the-jupyter-notebook","text":"Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the predict-insurance-expenses-regression-with-sklearn.ipynb notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Open the Jupyter notebook"},{"location":"machine-learning-sklearn/#run-the-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.","title":"Run the Jupyter notebook"},{"location":"machine-learning-sklearn/#load-and-prepare-dataset","text":"Section 2.0 Load the data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ).","title":"Load and Prepare Dataset"},{"location":"machine-learning-sklearn/#continue-to-run-the-notebook","text":"Finish running all of the cells. Carefully read all of the markdown comments to gain some understanding of how to build, test and evaluate a machine learning model using sklearn.","title":"Continue to run the notebook"},{"location":"project-setup/","text":"Workshop Setup \u00b6 Before we get started with the workshop, you will need to download some assets and setup your environment. This section is broken up into the following steps: Download Assets Create IBM Cloud Account and IBM Cloud Pak for Data services Create a Project Upload the data Associate a Watson Machine Learning Service instance to the project Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. 1. Download Assets \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following two zip files which you can download using the links below. For each line below, click on the [Download] link to get the file. If the link isn't working for you, try clicking the [Mirror] to get it from a backup server. You'll need these files in the next sections. Jumpstart your Journey with Cloud Pak for Data Project - [Download] | [Mirror] 2. Create IBM Cloud account and service \u00b6 We need to provision our Cloud Pak for Data as a Service instance. Cloud Pak for Data provides you with an integrated set of capabilities for collecting and organizing your data into a trusted, unified view, and then creating and scaling AI models across your business. Launch a web browser and navigate to IBM Cloud Pak for Data using the region closest to your location from the list below: US, Dallas EU, Frankfurt Japan, Tokyo You can then log in using your IBMid if you have one or create a new IBMid. If you do not have an IBMid, enter your email address and accept the terms checkbox in the Create a new IBM Cloud Account section. Then click the Next button to complete the process of creating a new account. If you are a returning user, click on the Log in with your IBMid link. > Note: If you are a returning user and you have watson services in a different region than the pre-selected one, you will see an error message telling you to select that region instead. See the FAQ section for help. The services required for IBM Cloud Pak for Data will be automatically provisioned for you. Once you see a message that says that the apps are ready to use, click on Go to IBM Cloud Pak for Data . 3. Create a project \u00b6 Import the Project \u00b6 In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Once you are on Cloud Pak for Data as a Service . Click on the (\u2630) navigation menu on the top left, expand Projects and click on the View all projects link. Click on the New project button on the top. Note: If you already have existing projects, your screen will look different from the screenshot below. In that case, click on the New project + button on the top right. Click on Create a project from a sample or file . Click on the browse link and in the file browser popup, navigate to where you downloaded the files for this lab. Then select the DDC2021-jumpstart-your-journey.zip . Give the project a name and optional description. You also need to provide an object storage instance for this project. If you have not previously created a Cloud Object Storage instance in your IBM Cloud account, you can create one now by clicking Add . Note: If you do have an existing storage service, select it from the drop down list and click the Create button. A new tab opens up, where you can create the Cloud Object Service. By default, a Lite (Free) plan will be selected. Scroll down and update the name of your Cloud Object Storage service if you wish, and click Create . The browser tab will automatically close when the Cloud Object Storage instance has been created. Back on IBM Cloud Pak for Data as a Service, click Refresh . The newly created Cloud Object Storage instance will now be displayed under \"Storage\". Click Create to finish creating the project. Once the project is succesfully created you will be brought to the project overview page ( Note:You may be presented with a project tour pop up window, go ahead and close it ) 4. Upload the data \u00b6 We'll use a data set from Kaggle for this workshop. You'll need to download it to your local machine, then upload to your project running in Cloud Pak for Data as a Service. Download the data to your laptop or PC by clicking this link . In your project, click on Add to project + and then choose the tile for Data . Alternately, under Data assets click New data asset + . Wait on this page until the upload completes. You should see the file insurance.csv under your Data assets . 5. Associate a Watson Machine Learning Service instance to the project \u00b6 You will need to associate a Watson Machine Learning service instance to your project in order to run Machine Learning experiments. Go to the Settings tab of your project and look for the Associated services section. Click on Add service and in the menu that opens up, click on Watson . Click the checkbox next to the Watson Machine Learning service instance that was created for you when you signed up for Cloud Pak for Data as a Service or the one you created in section 2. Click Associate service . Note: If you have multiple WatsonMachineLearning services, make sure you select the one that is in the same regions as is your Cloud Pak for Data as a service. You willsee a notification that the WatsonMachineLearning service was successfully associated with your project. Click on the X in the right top corner to close the pop up modal and go back to your project. You are now ready to move on to the next module of this course. FAQ \u00b6 Q1: I don't have all the services needed. A: In some rare cases, the services may not automatically provision for you. You can do that manually by following these instructions: Go the (\u2630) navigation menu on the top left corner of the Cloud Pak for Data UI. Expand Services and then click on Service instances . If you do not have an instance of Watson Machine Learning , or any service that you need, click on the Add service + button. Search or scroll until you find the tile for Machine Learning ,or whichever service you need, and click on it. Choose the same region as you chose for your Cloud Pak for Data as a Service platform, select the Free tier unless your organization has already used their 1 free tier, change the name and add tags if you like. The Default resource group should be correct, and then click Create . Q2: I get the That email address is already registered to an IBM Cloud account. messsage. A: You must already have an IBMid account. Follow the login link provided in the error message to login to your existing account. Q3: I get the Your Watson Studio, Watson Knowledge Catalog, and Watson Machine Learning Lite services must be created in the same service region. error. A: This means you have previously created some Watson services in a different region. To resolve this, go to the CP4DaaS Login page, select the region you had previously used and then login using the login link at the bottom right. Alternatively, you can create a new account and proceed as a new user to follow along. 6. Conclusion \u00b6 At this point we are done with this section. We have completed creating an IBM Cloud account, a Cloud Pak for Data as a Service instance, and the project that we will use in the rest of this workshop.","title":"Perform the steps to setup your project"},{"location":"project-setup/#workshop-setup","text":"Before we get started with the workshop, you will need to download some assets and setup your environment. This section is broken up into the following steps: Download Assets Create IBM Cloud Account and IBM Cloud Pak for Data services Create a Project Upload the data Associate a Watson Machine Learning Service instance to the project Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page.","title":"Workshop Setup"},{"location":"project-setup/#1-download-assets","text":"Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following two zip files which you can download using the links below. For each line below, click on the [Download] link to get the file. If the link isn't working for you, try clicking the [Mirror] to get it from a backup server. You'll need these files in the next sections. Jumpstart your Journey with Cloud Pak for Data Project - [Download] | [Mirror]","title":"1. Download Assets"},{"location":"project-setup/#2-create-ibm-cloud-account-and-service","text":"We need to provision our Cloud Pak for Data as a Service instance. Cloud Pak for Data provides you with an integrated set of capabilities for collecting and organizing your data into a trusted, unified view, and then creating and scaling AI models across your business. Launch a web browser and navigate to IBM Cloud Pak for Data using the region closest to your location from the list below: US, Dallas EU, Frankfurt Japan, Tokyo You can then log in using your IBMid if you have one or create a new IBMid. If you do not have an IBMid, enter your email address and accept the terms checkbox in the Create a new IBM Cloud Account section. Then click the Next button to complete the process of creating a new account. If you are a returning user, click on the Log in with your IBMid link. > Note: If you are a returning user and you have watson services in a different region than the pre-selected one, you will see an error message telling you to select that region instead. See the FAQ section for help. The services required for IBM Cloud Pak for Data will be automatically provisioned for you. Once you see a message that says that the apps are ready to use, click on Go to IBM Cloud Pak for Data .","title":"2. Create IBM Cloud account and service"},{"location":"project-setup/#3-create-a-project","text":"","title":"3. Create a project"},{"location":"project-setup/#import-the-project","text":"In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Once you are on Cloud Pak for Data as a Service . Click on the (\u2630) navigation menu on the top left, expand Projects and click on the View all projects link. Click on the New project button on the top. Note: If you already have existing projects, your screen will look different from the screenshot below. In that case, click on the New project + button on the top right. Click on Create a project from a sample or file . Click on the browse link and in the file browser popup, navigate to where you downloaded the files for this lab. Then select the DDC2021-jumpstart-your-journey.zip . Give the project a name and optional description. You also need to provide an object storage instance for this project. If you have not previously created a Cloud Object Storage instance in your IBM Cloud account, you can create one now by clicking Add . Note: If you do have an existing storage service, select it from the drop down list and click the Create button. A new tab opens up, where you can create the Cloud Object Service. By default, a Lite (Free) plan will be selected. Scroll down and update the name of your Cloud Object Storage service if you wish, and click Create . The browser tab will automatically close when the Cloud Object Storage instance has been created. Back on IBM Cloud Pak for Data as a Service, click Refresh . The newly created Cloud Object Storage instance will now be displayed under \"Storage\". Click Create to finish creating the project. Once the project is succesfully created you will be brought to the project overview page ( Note:You may be presented with a project tour pop up window, go ahead and close it )","title":"Import the Project"},{"location":"project-setup/#4-upload-the-data","text":"We'll use a data set from Kaggle for this workshop. You'll need to download it to your local machine, then upload to your project running in Cloud Pak for Data as a Service. Download the data to your laptop or PC by clicking this link . In your project, click on Add to project + and then choose the tile for Data . Alternately, under Data assets click New data asset + . Wait on this page until the upload completes. You should see the file insurance.csv under your Data assets .","title":"4. Upload the data"},{"location":"project-setup/#5-associate-a-watson-machine-learning-service-instance-to-the-project","text":"You will need to associate a Watson Machine Learning service instance to your project in order to run Machine Learning experiments. Go to the Settings tab of your project and look for the Associated services section. Click on Add service and in the menu that opens up, click on Watson . Click the checkbox next to the Watson Machine Learning service instance that was created for you when you signed up for Cloud Pak for Data as a Service or the one you created in section 2. Click Associate service . Note: If you have multiple WatsonMachineLearning services, make sure you select the one that is in the same regions as is your Cloud Pak for Data as a service. You willsee a notification that the WatsonMachineLearning service was successfully associated with your project. Click on the X in the right top corner to close the pop up modal and go back to your project. You are now ready to move on to the next module of this course.","title":"5. Associate a Watson Machine Learning Service instance to the project"},{"location":"project-setup/#faq","text":"Q1: I don't have all the services needed. A: In some rare cases, the services may not automatically provision for you. You can do that manually by following these instructions: Go the (\u2630) navigation menu on the top left corner of the Cloud Pak for Data UI. Expand Services and then click on Service instances . If you do not have an instance of Watson Machine Learning , or any service that you need, click on the Add service + button. Search or scroll until you find the tile for Machine Learning ,or whichever service you need, and click on it. Choose the same region as you chose for your Cloud Pak for Data as a Service platform, select the Free tier unless your organization has already used their 1 free tier, change the name and add tags if you like. The Default resource group should be correct, and then click Create . Q2: I get the That email address is already registered to an IBM Cloud account. messsage. A: You must already have an IBMid account. Follow the login link provided in the error message to login to your existing account. Q3: I get the Your Watson Studio, Watson Knowledge Catalog, and Watson Machine Learning Lite services must be created in the same service region. error. A: This means you have previously created some Watson services in a different region. To resolve this, go to the CP4DaaS Login page, select the region you had previously used and then login using the login link at the bottom right. Alternatively, you can create a new account and proceed as a new user to follow along.","title":"FAQ"},{"location":"project-setup/#6-conclusion","text":"At this point we are done with this section. We have completed creating an IBM Cloud account, a Cloud Pak for Data as a Service instance, and the project that we will use in the rest of this workshop.","title":"6. Conclusion"},{"location":"python3/","text":"Learning Python3 \u00b6 What is Python? \u00b6 From the Python.org documentation : Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together. Python's simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse. The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed. Often, programmers fall in love with Python because of the increased productivity it provides. Since there is no compilation step, the edit-test-debug cycle is incredibly fast. Debugging Python programs is easy: a bug or bad input will never cause a segmentation fault. Instead, when the interpreter discovers an error, it raises an exception. When the program doesn't catch the exception, the interpreter prints a stack trace. A source level debugger allows inspection of local and global variables, evaluation of arbitrary expressions, setting breakpoints, stepping through the code a line at a time, and so on. The debugger is written in Python itself, testifying to Python's introspective power. On the other hand, often the quickest way to debug a program is to add a few print statements to the source: the fast edit-test-debug cycle makes this simple approach very effective. You might think that Python is only for developers and people with computer science degrees. However, Python is great for beginners, even those with little coding experience because it\u2019s free, open source, and runs on any platform. The Python packages documentation is great, and after an introductory course , you have a good foundation to build on. For Data Scientists, Python has become an invaluable tool. This notebook will help you get started or review the basics of Python. Getting Started with Jupyter Notebooks \u00b6 Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. In this tutorial, you use IBM Watson Studio to run a notebook. For this, you need a free IBM Cloud account. The following steps show you how sign up and get started. When you have the notebook up and running, you can go through the notebook. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. In this workshop we will use IBM Watson Studio to run a notebook. For this you will need an IBM Cloud account. The following steps will show you how sign up and get started. When you have the notebook up and running we will go through the notebook. If you have not already done so, make sure that you do the work for your project setup Load and Run a Notebook \u00b6 Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the learning-python-3 notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it. Run the Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ).","title":"Getting started with Python3"},{"location":"python3/#learning-python3","text":"","title":"Learning Python3"},{"location":"python3/#what-is-python","text":"From the Python.org documentation : Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together. Python's simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse. The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed. Often, programmers fall in love with Python because of the increased productivity it provides. Since there is no compilation step, the edit-test-debug cycle is incredibly fast. Debugging Python programs is easy: a bug or bad input will never cause a segmentation fault. Instead, when the interpreter discovers an error, it raises an exception. When the program doesn't catch the exception, the interpreter prints a stack trace. A source level debugger allows inspection of local and global variables, evaluation of arbitrary expressions, setting breakpoints, stepping through the code a line at a time, and so on. The debugger is written in Python itself, testifying to Python's introspective power. On the other hand, often the quickest way to debug a program is to add a few print statements to the source: the fast edit-test-debug cycle makes this simple approach very effective. You might think that Python is only for developers and people with computer science degrees. However, Python is great for beginners, even those with little coding experience because it\u2019s free, open source, and runs on any platform. The Python packages documentation is great, and after an introductory course , you have a good foundation to build on. For Data Scientists, Python has become an invaluable tool. This notebook will help you get started or review the basics of Python.","title":"What is Python?"},{"location":"python3/#getting-started-with-jupyter-notebooks","text":"Instead of writing code in a text file and then running the code with a Python command in the terminal, you can do all of your data analysis in one place. Code, output, tables, and charts can all be edited and viewed in one window in any web browser with Jupyter Notebooks . As the name suggests, this is a notebook to keep all of your ideas and data explorations in one place. In this tutorial, you use IBM Watson Studio to run a notebook. For this, you need a free IBM Cloud account. The following steps show you how sign up and get started. When you have the notebook up and running, you can go through the notebook. Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. In this workshop we will use IBM Watson Studio to run a notebook. For this you will need an IBM Cloud account. The following steps will show you how sign up and get started. When you have the notebook up and running we will go through the notebook. If you have not already done so, make sure that you do the work for your project setup","title":"Getting Started with Jupyter Notebooks"},{"location":"python3/#load-and-run-a-notebook","text":"Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the learning-python-3 notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it.","title":"Load and Run a Notebook"},{"location":"python3/#run-the-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ).","title":"Run the Jupyter notebook"}]}